## Using btrfs for keeping many similar clones of the wiki


### General info

See [btrfs] for btrfs explanation and [started] for basic commands.

[btrfs]: https://btrfs.wiki.kernel.org/index.php/SysadminGuide
[started]: https://btrfs.wiki.kernel.org/index.php/Getting_started#Basic_Filesystem_Commands


#### Basic Commands

    mount -t btrfs /dev/sdb3 /mnt/btrfs
    
    btrfs filesystem show   # show allocation

Create subvolume and a snapshot of root:

    btrfs subvolume create /mnt/btrfs/new_subvol
    btrfs subvolume snapshot /mnt/btrfs /mnt/btrfs/snapshot_of_root
    btrfs subvolume snapshot root root_snapshot_2011_01_11


Mount a subvolume and a snapshot:

    mount -t btrfs -o subvol=new_subvol /dev/sdb3 /mnt/new_subvol
    mount -t btrfs -o subvol=snapshot_of_root /dev/sdb3 /mnt/snap

Rolling back a snapshot:
    
    umount /home
    mount -o defaults,subvol=home_snapshot_A /dev/sda /home
    btrfs subvolume delete /media/btrfs/home  



#### Snapshots

A snapshot is simply a subvolume that shares its data (and metadata)
with some other subvolume, using btrfs's COW capabilities. Once a
[writable] snapshot is made, there is no difference in status between
the original subvolume, and the new snapshot subvolume. To roll back
to a snapshot, unmount the modified original subvolume, and mount the
snapshot in its place. At this point, the original subvolume may be
deleted if wished. Since a snapshot is a subvolume, snapshots of
snapshots are also possible.


### Usage for mwiki

#### Initial use for fast rollback replacing rsync

- variable MW_BTRFS tells to use btrfs instead of rsync,
  and (if nonempty) contains the path to the root btrfs filesystem
- the public_repo lives in a subvolume `$REPO_NAME`
- instead of rsync to a sandbox, a fresh snapshot is created each time:

        my $snapshot = mkdtemp("$MW_BTRFS/$REPO_NAME/snapshot_XXXXXX");
        btrfs subvolume snapshot $backend $MW_BTRFS/$REPO_NAME/snapshot
    
  a commit is done, and used for verification. If successful, it is
  rsync-ed to the public_repo, and put into the list of deletable
  snapshots (some cron script checks that list). The snapshot name
  will have to be passed as a parameter to the pre/post commit

#### Use for many users

- again, variable MW_BTRFS tells to use btrfs

### Experiments

#### Space: Summary

We looked at the degree of sharing due to copy-on-write for the dataset
"mizar text unpacked", containing 53 versions, from version 7.5.01\_4.32.908
to 7.11.07'_4.160.1126. Each version is stored by cloning the previous
version, and updating only those files that have a different size in the
new version. 

Total disk usage of these versions on ext4 is 11958mb (average 226mb).

Space usage on btrfs with COW cloning, without compression:
disk usage: 6219mb / including filesystem overhead: 7956mb

With fs overhead, this means that this technique saves about 34% disk
space with respect to the original, unshared, data.

#### Space: Measurements

- Input data: "mizar text unpacked"
- Mizar versions: 53 versions, from 7.5.01\_4.32.908 to 7.11.07\_4.160.1126
- Disk usages (using "du"):
-  source, on ext4:  total: 11958m (average single version 225.6m)
-  target, on btrfs: total: 11786m (average single version 222.4m) 

Apparent sizes (using "du --apparent-size") (why are these not equal?
different apparent sizes for the directories??):

- source, on ext4:  total: 11256m (average single version 212.4m)
- target, on btrfs: total: 11248m (average single version 212.2m)

        $ btrfs fi show test
        Label: 'test'  uuid: 270f1457-25c6-43c5-b6f9-f0388413deea
        Total devices 1 FS bytes used 6.07GB
        devid    1 size 10.00GB used 7.77GB path /dev/xvdb

Btrfs v0.19

        $ df -B 1m /mnt/data
        Filesystem           1M-blocks      Used Available Use% Mounted on
        /dev/xvdb                10240      6219      3470  65% /mnt/data

(N.B.  6219/1024 = 6.07)

#### Time: Summary

The time required to create a clone, both an empty one and a copy of an
existing 30G subvolume, is constant at 0.03s up until 10k clones. Around
50k clones, it starts slowing down.

The slowdown is less pronounced if we distribute the clones over
subdirectories according to the first two characters of their names ("00"
to "ff" -- the clone names are sha1sums), so as to avoid a directory with a
very large number of entries. However, even with these prefix-subdirectories,
cloning still slows down beyond 10k clones, so the slowdown also depends on
the number of clones per filesystem.

The time required to destroy a clone is constant at 0.14s, at least until
10k clones.

#### Time: Measurements

- creating empty subvolumes

<table>
  <thead>
    <tr>
      <th>#clones</th>
      <th>time (sec/clone)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>10</td>
      <td>0.020</td>
    </tr>
    <tr>
      <td>100</td>
      <td>0.020</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>0.022</td>
    </tr>
    <tr>
      <td>5000</td>
      <td>0.024</td>
    </tr>
    <tr>
      <td>10000</td>
      <td>0.024</td>
    </tr>
  </tbody>
</table>

- creating clones of 30G subvolume (unpacked mwiki.tar.gz)

<table>
  <thead>
    <tr>
      <th>#clones</th>
      <th>time (sec/clone)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>10</td>
      <td>0.024</td>
    </tr>
    <tr>
      <td>100</td>
      <td>0.023</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>0.025</td>
    </tr>
    <tr>
      <td>5000</td>
      <td>0.025</td>
    </tr>
    <tr>
      <td>10000</td>
      <td>0.029</td>
    </tr>
    <tr>
      <td>50000</td>
      <td>0.036</td>
    </tr>
  </tbody>
</table>
   
<table>
  <thead>
    <tr>
      <th>#clones</th>
      <th>time (sec/clone)</th>
    </tr>
  </thead>
  <tbody>
<tr>
<td>100</td>
<td>0.024</td>
<tr>
<tr>
<td>1000</td>
<td>(?)</td>
</tr>
<tr>
<td>10000</td>
<td>0.027</td>
</tr>
<tr>
<td>50000</td>
<td>0.036</td>
</tr>
<tr>
<td>100000</td>
<td>0.051</td>
</tr>
<tr>
<td>200000</td>
<td>0.070</td>
</tr>
</tbody>
</table>

- creating clones of 30G subvolume *with one level of prefix dir (00/ .. ff/)*

<table>
  <thead>
    <tr>
      <th>
        <td>#clones</td>
        <td>time (sec/clone)</td>
      </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>100</td>
      <td>0.024</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>0.025</td>
    </tr>
    <tr>
      <td>10000</td>
      <td>0.028</td>
    </tr>
    <tr>
      <td>100000</td>
      <td>0.039</td>
    </tr>
  </tbody>
</table>

- destroying empty subvolumes

<table>
  <thead>
    <tr>
      <th>
        <td>#clones</td>
        <td>time (sec/clone)</td>
      </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>10</td>
      <td>0.129</td>
    </tr>
    <tr>
      <td>100</td>
      <td>0.139</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>0.137</td>
    </tr>
    <tr>
      <td>5000</td>
      <td>0.138</td>
    </tr>
  </tbody>
</table>

- destroying clones of 30G subvolume

<table>
<thead>
<tr>
<th>
<td>#clones</td>
<td>time (sec/clone)</td>
</th>
</thead>
<tbody>
<tr>
<td>10</td>
<td>0.119</td>
</tr>
<tr>
<td>100</td>
<td>0.140</td>
</tr>
<tr>
<td>1000</td>
<td>0.126</td>
</tr>
</tbody>
</table>

<table>
<thead>
<tr>
<th>
<td>#clones</td>
<td>time (sec/clone)</td>
</th>
</thead>
<tbody>
<tr>
<td>100</td>
<td>0.128</td>
</tr>
<tr>
<td>1000</td>
<td>0.137</td>
</tr>
<tr>
<td>10000</td>
<td>0.139</td>
</tr>
</tbody>
</table>

- untarring 30G "mwiki.tar.gz" into subvolume

        real    6m47.861s
        user    2m10.650s
        sys     2m52.360s
